# -*- coding: utf-8 -*-
"""ws4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SrLP334f5Haz63_AHk62BiIhAy_TgsG3

# CIS 1920 Worksheet 4 (2 pts total)

**Due Wednesday March 15, 2023 11:59 pm EST**

## Objectives

- Familiarization with Colab and Jupyter notebooks
- Practice building ipywidgets
- Explore a real-world machine learning dataset using [Pandas](https://pandas.pydata.org/) and [Seaborn](https://seaborn.pydata.org/)
- Consider the use of machine learning in societal contexts

**Note**: Just like the other worksheets, this worksheet is completion-graded, meaning that you will receive full credit if all the TODOs are filled in and all the discussion questions are answered, regardless of correctness.

## Setup

**First, make a copy of this Colab to your Google Drive by clicking "Copy to Drive" in the upper left or `File -> save a copy in Drive` so you can save any changes you make.**

Google Colab environments have many of the typical data science and machine learning packages already pre-installed, so no need for `pip install` commands. Run the cell below using `Ctrl-Enter` to import modules:
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ipywidgets import interact

"""## Machine learning bias: COMPAS recidivism

Some of the most controversial topics in modern machine learning (ML) research concern the *bias* (note that this is different than the technical term of bias in the bias-variance tradeoff) and *fairness* of ML models. You may have heard about various concerning results in [facial image depixelization](https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias) and [image classification](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/) where the machine learning algorithms exhibited bias in their output.

A fundamental question is the definition of what it means for a ML model to be "biased" or "unfair", and many competing definitions have been proposed. Here we will explore a few definitions of fairness using the dataset that has been a mainstay in the debate and research into bias and fairness in machine learning: the COMPAS recidivism dataset.

In 2016, Propublica [released an article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) on a proprietary machine learning model called Correctional Offender Management Profiling for Alternative Sanctions (COMPAS). COMPAS takes as input a convicted individual's relevant information, such as demographics and prior criminal history, and outputs a risk score for **recidivism**: their probability of re-offending. COMPAS was used across the country by judges as additional information for them to use when sentencing convicted individuals. Propublica contended that COMPAS was biased against African-Americans, giving them much higher risk scores than whites with similar characteristics, which has been [refuted](http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf) by the creators of COMPAS. 

Part of the issue stems from the fact that COMPAS is proprietary, so researchers are only able to see what predictions COMPAS makes and are unable to examine **how** the algorithm makes its predictions. Since the Propublica's article release, the COMPAS data has been utilized across ML research in bias, fairness, and interpretability, with debates on fairness and the correctness of Propublica's analysis [still ongoing](https://hdsr.mitpress.mit.edu/pub/7z10o269/release/7).

## Loading the data

A simplified version of the COMPAS dataset can be downloaded at [this link](https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv). Jupyter notebooks allow you to run bash commands by beginning a line with `!`, so let's use `wget` to directly download the data by running the cell below:
"""

! wget https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv

"""Confirm you have the csv file downloaded correctly by running the cell loading the COMPAS data as a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html ):"""

compas = pd.read_csv("compas-scores-two-years.csv")

"""The three columns of the dataset we will explore in this worksheet are `race`, `decile_score`, and `is_recid`. `race` is a string indicating the individual's race. `decile_score` is the **predicted** recidivism risk on a scale of 1 to 10 made by the COMPAS algorithm, with 10 being the highest risk. `is_recid` is a binary indicator of whether the individual actually recidivated (i.e. re-committed a crime after serving their sentence).

We can take a peek of what the data look like by using the `DataFrame.head()` method:
"""

compas[['race', 'decile_score', 'is_recid']].head(10)

"""## Task 1: Interactive Visualization

Let's first explore the dataset a bit using the ipywidgets functionality we discussed in class. Below we have provided a function `plot_risk_vs_races()` that plots the `decile_score` counts for two given races. Your job is to use the `interact()` function to make the plot interactive, with two dropdown menus for selecting the races to compare among `['Other', 'African-American', 'Caucasian', 'Hispanic', 'Native American', 'Asian']`.

"""

def plot_risk_vs_races(race1, race2):
    """
    Plots a seaborn barplot comparing the percentages of individuals in each
    decile score for the given races.

    Args:
        race1 (str): the first race to compare
        race2 (str): the second race to compare
    Returns:
        None, but displays a plot
    """
    fig, ax = plt.subplots()
    race_df = compas[compas['race'].isin([race1, race2])]

    race_df = race_df.groupby(['decile_score', 'race'])['id'].count()
    races = race_df.groupby(['race']).sum()
    race_df = race_df.div(races, level="race")
    race_df = race_df.reset_index()

    sns.barplot(x='decile_score', hue='race', y='id', data=race_df, ax=ax)
    ax.set_ylabel("percentage of population in decile, per race")

races = compas['race'].unique()

# TODO: interact function for the two race parameters
# interact(plot_risk_vs_races, ...)

interact(plot_risk_vs_races, race1=races, race2=races)

"""## Discussion question 1 [0.5 pts]

**Write your response directly in this markdown cell.**

1. What trends in the percentages of the total population per race do you see as the decile score increases for Caucasians, and how does this trend compare versus the ones you see in Asians, Hispanics, and African-Americans?

**Your response**: 

1. For caucasians, there is an almost exponential decay pattern to the bar graph where the highest percentage of population is at a decile score of 1 and the percentage steadily declines as the score increases. Asians follow a similar pattern but the bars are less consistent in their decrease and there is a greater percentage of the population with a decile score of 1. Hispanics have an almost identical pattern to caucasians, except there may be a greater percentage of the population distributed along bars beyond a decile score of 1. African Americans have the most different distribution- it is essentially flat and evenly distributed among the decile scores.

## Task 2: Fairness definiton \#1: calibration within groups [0.5 pts]

One definition of fairness is **calibration within groups**: for this data, the groups are well-calibrated if for each decile score of recidivism risk, the fraction of individuals who actually re-offend (given by the `is_recid` column) is the same for any two groupings of the data.

Here, let's look at the calibration within groups for Caucasians and African-Americans by filling out the `sns.barplot` call with `decile_score` on the x-axis, `is_recid` on the y-axis, and `race` as the hue grouping.
"""

# TODO fill out the sns.barplot call to visualize calibration within groups
sel_df = compas[compas['race'].isin(['Caucasian', 'African-American'])]
sns.barplot(x=sel_df['decile_score'], y=sel_df['is_recid'], data=sel_df, hue=sel_df['race'])

"""The black error bars indicate 95% confidence intervals, so if the error bars overlap for Caucasians and African-Americans for every decile score, we can say that the algorithm is fair under calibration within groups, and if they do not overlap, then the algorithm is unfair under this definition.

## Task 3: Fairness definition \#2: Balance for the negative class [0.5 pts]

Another definition of fairness is **balance for the negative class**: the average score of individuals in one group who don't recidivate should be the same average score of the inidividuals in the other group who don't recidivate. This means that the assignment of score should not be *more inaccurate* for non-recidivating instances in one group than the other.

Let's compute the balance for the negative class here by filtering the `compas` DataFrame to only individuals who did not recidivate and then comparing the average scores for African-Americans and Causasians who did not recidivate.
"""

# filter to the two races
sel_df = compas[compas['race'].isin(['Caucasian', 'African-American'])]

# TODO filter to only individuals with is_recid == 0
sel_df = sel_df[sel_df['is_recid'] == 0]
# TODO generate a barplot with race on the x-axis, decile_score on the y-axis 
fig, ax = plt.subplots()
sns.barplot(x=sel_df['race'], y=sel_df['decile_score'], data=sel_df, ax=ax);
ax.set_title("Average scores of individuals who *did not* recidivate");

# TODO groupby race then call describe() on the 'decile_score' column to display
# descriptive statistics
display(sel_df.groupby('race')['decile_score'].describe())

"""Again, the black error bars indicate 95% confidence intervals, so if the error bars overlap for the average decile score of non-recidivating Caucasians and African-Americans, then the algorithm is fair under balance for the negative class, and if the error bars do not overlap, then the algorithm is unfair under this definition.

## Discussion questions 2 and 3 [0.5 pts]

**Write your response directly in this markdown cell.**

2. Under which definition of fairness (if any) is the COMPAS algorithm "fair"?

3. It can be [mathematically proven](https://arxiv.org/abs/1609.05807) that a machine learning classifier cannot simultaneously satisfy both **balance for the negative class** and **calibration within groups**, unless it has perfect accuracy. This general dilemma with competing mathematical fairness definitions has been called the ["impossibility of fairness"](https://arxiv.org/abs/1609.07236).

    In your opinion, should these mathematical definitions of fairness even have a role in societal contexts and decision making?

**Your response**:

2. It is only fair under the 'calibration within groups' definition of fairness. This is because the error bars overlap for every decile score. However, for the 'balance for the negative class' definition, the error bars are very far from overlapping, showing that the algorithm has bias.

3. I think that they do provide valuable insights as to the bias in algorithmic design, but I would not give them much weight in societal contexts. I think this is because they mask very deep-rooted societal issues that can really only be solved and discussed in a qualitative context. For example, despite what the algorithm may imply, there is a disproportionate perception that certain races are more heavily involved in crime than others. Even if, in some cases, we can mathematically demonstrate that some of these perceptions are warranted, we fail to acknowledge the underlying issues behind that- like class issues and structural inequalities. As such, we may be drawing a correlation between factors that have one, if not many, confounding variables.

## Submission

Afer you have completed the worksheet, download the Colab notebook as a `ws4.py` file by going to `File -> Download .py`. Then submit your `ws4.py` file to Gradescope. Note that there is no autograder for this submission.

## Attribution

This worksheet was adapted from the [Econ-ML blog post](http://econ-neural.net/kleinberg-mullainathan.html) on "Inherent Trade-Offs in the Fair Determination of Risk Scores."
"""